const OpenAI = require("openai");

// Configuraci√≥ del client per a IA LOCAL
const openai = new OpenAI({
  baseURL: "http://pi_llm:8080/v1", // MODIFICAT: Connecta amb 'pi_llm' (nom real del contenidor)
  apiKey: "sk-no-key-required",  // La IA local no necessita clau real
  timeout: 30 * 60 * 1000,       // NOU: 30 minuts de timeout (augmentat per si va lent)
});

/**
 * Comprova si el contenidor de la IA est√† disponible.
 * Ho intenta 5 vegades abans de rendir-se.
 */
async function checkConnection(retries = 100) {
    const url = "http://pi_llm:8080/health"; // Endpoint de salut de llama.cpp
    console.log(`üîç [aiService] Comprovant connexi√≥ amb IA Local (${url})...`);
    
    for (let i = 0; i < retries; i++) {
        try {
            const response = await fetch(url);
            if (response.ok) {
                console.log("‚úÖ [aiService] IA Local ONLINE (Port 8080 obert).");
                
                // NOU: Test real de generaci√≥ per confirmar que "pensa"
                console.log("üß™ [aiService] Fent prova de generaci√≥ r√†pida (Warm-up)...");
                try {
                    await openai.chat.completions.create({
                        model: "default-model",
                        messages: [{ role: "user", content: "Test" }],
                        max_tokens: 1
                    });
                    console.log("üöÄ [aiService] TEST SUPERAT! La IA est√† generant text correctament.");
                } catch (e) {
                    console.warn("‚ö†Ô∏è [aiService] El test de generaci√≥ ha fallat (potser est√† carregant model):", e.message);
                }

                return true;
            }
            // Si respon per√≤ no √©s OK (ex: 503 Loading...), avisem
            console.warn(`‚ö†Ô∏è [aiService] La IA est√† carregant models (Status: ${response.status})...`);
        } catch (error) {
            console.warn(`‚ö†Ô∏è [aiService] Intent ${i+1}/${retries} fallit: ${error.message}`);
        }
        // MOGUT: Esperem 3s SEMPRE si no hem acabat, tant si falla la xarxa com si est√† carregant
        if (i < retries - 1) await new Promise(r => setTimeout(r, 3000));
    }
    console.error("‚ùå [aiService] IMPOSSIBLE CONNECTAR AMB LA IA. Revisa que el contenidor 'pi_llm' estigui enc√®s i a la mateixa xarxa.");
    return false;
}

/**
 * Genera un resum utilitzant IA LOCAL (sense streaming HTTP directe).
 * Retorna el text complet quan acaba.
 * @param {string} role - 'docent' o 'orientador'
 * @param {function} onProgress - Callback opcional (textParcial, percentatge)
 */
async function generateSummaryLocal(text, role, onProgress) {

  // Retallem el text per no saturar el context del model
  const MAX_CHARS = 6000; // Redu√Øt encara m√©s per garantir resposta r√†pida en CPU
  const truncatedText = text.length > MAX_CHARS ? text.substring(0, MAX_CHARS) + "..." : text;

  let currentProgress = 0;
  
  // FASE 1: LECTURA
  // Eliminem la simulaci√≥. No enviarem progr√©s fals. El frontend mostrar√† "Llegint..." sense barra o amb barra indeterminada.

  // --- SELECCI√ì DE PROMPT SEGONS ROL ---
  let systemPrompt = "";
  
  if (role === 'orientador') {
    // PROMPT PER A ORIENTADORS
    systemPrompt = `Ets un assistent expert per a orientadors educatius.
      OBJECTIU: Extreure informaci√≥ clau per a l'orientaci√≥ i seguiment de l'alumne.
      
      ESTRUCTURA OBLIGAT√íRIA (5 SECCIONS):
      1. PERFIL DE L'ALUMNE (Dades personals i acad√®miques)
      2. DIAGN√íSTIC (Problemes detectats)
      3. JUSTIFICACI√ì DEL PI (Motiu del pla)
      4. ORIENTACI√ì A L'AULA (Pautes d'actuaci√≥)
      5. MAT√àRIES (Adaptacions curriculars)

      FORMAT: "Idea clau molt breu. [[Detall: Text original...]]"`;
  } else {
    // PROMPT PER A DOCENTS (Defecte)
    systemPrompt = `Ets un assistent expert per a docents.
      OBJECTIU: Facilitar informaci√≥ pr√†ctica per a l'aula i l'avaluaci√≥.
      
      ESTRUCTURA OBLIGAT√íRIA (5 SECCIONS):
      1. PERFIL DE L'ALUMNE (Dades personals i acad√®miques)
      2. DIAGN√íSTIC (Problemes detectats)
      3. ORIENTACI√ì A L'AULA (Pautes d'actuaci√≥)
      4. ASSIGNATURES (Adaptacions espec√≠fiques)
      5. CRITERIS D'AVALUACI√ì (Com avaluar)

      FORMAT: "Idea clau molt breu. [[Detall: Text original...]]"`;
  }

  const messages = [
    {
      role: "system",
      content: `${systemPrompt}
      
      INSTRUCCIONS CR√çTIQUES DE FORMAT I CONTINGUT:
      1. T√çTOLS: Fes servir EXACTAMENT els t√≠tols de secci√≥ llistats amunt (en maj√∫scules). S√≥n OBLIGATORIS.
      2. FORMAT: Separa clarament cada secci√≥ amb un salt de l√≠nia.
      3. CONTINGUT COMPLET: Has d'incloure TOTA la informaci√≥ rellevant que trobis al document per a cada secci√≥. No resumeixis tant que es perdin dades.
      4. ESTIL LLISTA: Fes servir guions (-) o asteriscs (*) per a cada punt. Exemple: "- M√©s temps als ex√†mens". Evita par√†grafs llargs.
      5. NO COPI√èS LLISTES DE FORMULARI: Si veus opcions com "1r ESO, 2n ESO...", tria nom√©s la marcada o vigent.
      6. DETALLS: Extreu la frase literal clau del PDF dins dels claud√†tors [[Detall: ...]].
      
      Processa tot el text proporcionat.`
    },
    {
      role: "user",
      content: `Analitza aquest PI i extreu-ne la informaci√≥ rellevant:\n\n${truncatedText}`
    }
  ];

  try {
    console.log(`ü§ñ [aiService] Enviant petici√≥ a IA Local (http://pi_llm:8080/v1)...`);
    const completion = await openai.chat.completions.create({
      model: "default-model", // El nom √©s indiferent per a llama.cpp
      messages: messages,
      temperature: 0.1,
      max_tokens: 1000, // LIMITAT: Evita que s'enrotlli (la "chapa") i fa que acabi abans
      stream: true, // ACTIVEM STREAMING per veure el progr√©s
    });

    console.log("ü§ñ [aiService] Connexi√≥ establerta amb LLM! Esperant el primer token (Fase de Lectura/Pre-fill)...");

    let fullText = "";
    // Seccions esperades per calcular el progr√©s (aprox 20% per secci√≥)
    // MODIFICAT: Keywords actualitzades segons els nous prompts (Docent/Orientador)
    const sections = ["PERFIL", "DADES", "DIAGN√íSTIC", "ORIENTACI√ì", "ADAPTACIONS", "MAT√àRIES", "ASSIGNATURES", "CRITERIS", "JUSTIFICACI√ì"];
    let isFirst = true;
    let chunkCount = 0;

    for await (const chunk of completion) {
        // FASE 2: ESCRIPTURA (Reset a 0% -> 100%)
        if (isFirst) {
            console.log("ü§ñ [aiService] Primer token rebut! Comen√ßa la generaci√≥ de text.");
            isFirst = false;
            currentProgress = 0; // Reiniciem la barra per a la fase d'escriptura
        }

        chunkCount++;
        const content = chunk.choices[0]?.delta?.content || "";
        
        // Log de "batec" cada 10 chunks per veure que est√† viu a la terminal (M√©s freq√ºent)
        if (chunkCount % 10 === 0) {
            console.log(`... generant (${chunkCount} tokens)`); // M√©s visible als logs de Docker
        }

        fullText += content;

        if (onProgress) {
            // C√†lcul simple de progr√©s: Quantes seccions hem trobat ja?
            let foundCount = 0;
            sections.forEach(s => {
                if (fullText.includes(s)) foundCount++;
            });
            
            // C√†lcul de progr√©s d'escriptura (0 a 100)
            // MODIFICAT: Ajustem a 800 tokens (resum curt) perqu√® la barra sigui realista
            // (chunkCount / 8) -> 800 tokens = 100%
            const chunkProgress = (chunkCount / 8); 
            const sectionProgress = foundCount * 5; // M√©s pes a les seccions per compensar
            
            let writeProgress = chunkProgress + sectionProgress;
            
            // Enviem text ple -> Servidor marca "GENERANT..."
            // AWAIT IMPORTANT: Esperem que s'actualitzi la BD abans de continuar per evitar race conditions al final
            await onProgress(fullText, Math.min(Math.floor(writeProgress), 99));
        }
    }

    console.log(`ü§ñ [IA Local] Generaci√≥ finalitzada amb √®xit. Longitud: ${fullText.length} car√†cters.`);
    return fullText;
  } catch (error) {
    console.error("‚ùå Error IA Local:", error);
    throw new Error("Error connectant amb el contenidor d'IA Local.");
  }
}

module.exports = { generateSummaryLocal, checkConnection };