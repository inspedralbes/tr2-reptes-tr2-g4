const OpenAI = require("openai");

// Configuraci√≥ del client per a OLLAMA
const openai = new OpenAI({
    baseURL: "http://pi_llm:11434/v1", // Port d'Ollama
    apiKey: "ollama",  // Ollama requereix una string qualsevol
    timeout: 60 * 60 * 1000, // 1 hora timeout
});

const MODEL_NAME = "pimodel"; // Nom intern que donarem al teu model dins d'Ollama

const fs = require('fs');
const path = require('path');

/**
 * Funci√≥ robusta per inicialitzar la IA.
 * Mant√© el contenidor viu (no crash) mentre intenta connectar i configurar Ollama.
 */
async function initializeAI() {
    const modelFileName = "Llama-3.2-3B-Instruct-Q4_K_M.gguf";
    // PATHS:
    // internalPath: On ho veiem nosaltres (Backend Node)
    // ollamaPath: On ho veu Ollama (Volum Docker muntat a /models)
    const internalPath = `/app/models/${modelFileName}`;
    const ollamaPath = `/models/${modelFileName}`;
    const ollamaHost = "http://pi_llm:11434";

    console.log("üöÄ [aiService] INICIANT SISTEMA D'INTEL¬∑LIG√àNCIA ARTIFICIAL...");

    // 1. VERIFICACI√ì DE FITXER
    if (!fs.existsSync(internalPath)) {
        console.error(`‚ùå [aiService] ERROR FATAL: No es troba el fitxer .gguf a: ${internalPath}`);
        console.error("   Assegura't que l'has posat a tr2-reptes-tr2-g4/Incorporacio_PI/backend/models/");
        // En aquest cas no podem fer res, per√≤ no fem crash del tot per deixar el servidor web actiu (upload files)
        return false;
    }

    // BUCLE INFINIT DE CONNEXI√ì (El que demana l'usuari: "que no reinicie, que ho intenti")
    while (true) {
        try {
            // A. PING OLLAMA
            // Intentem veure si Ollama est√† despert
            try {
                const health = await fetch(`${ollamaHost}/api/tags`);
                if (!health.ok) throw new Error(`Ollama status ${health.status}`);
            } catch (netErr) {
                console.warn("‚è≥ [aiService] Esperant a Ollama (pi_llm)...");
                await new Promise(r => setTimeout(r, 3000));
                continue; // Tornem a l'inici del bucle
            }

            console.log("‚úÖ [aiService] Ollama connectat!");

            // NOU: Verificar versi√≥ per debug
            try {
                const verRes = await fetch(`${ollamaHost}/api/version`);
                const verData = await verRes.json();
                console.log(`‚ÑπÔ∏è [aiService] Versi√≥ Ollama: ${verData.version}`);
            } catch (ignore) { }

            // B. CHECK/CREATE MODEL
            // Comprovem si el model ja existeix
            const tagsRes = await fetch(`${ollamaHost}/api/tags`);
            const tagsData = await tagsRes.json();
            const exists = tagsData.models?.some(m => m.name === MODEL_NAME || m.name === `${MODEL_NAME}:latest`);

            if (exists) {
                console.log(`‚úÖ [aiService] El model '${MODEL_NAME}' JA EST√Ä CARREGAT.`);
                break; // Sortim del bucle, tot correcte!
            }

            // Si no existeix, el creem
            console.log(`‚öôÔ∏è [aiService] EL MODEL NO EXISTEIX. CREANT-LO ARA...`);
            console.log(`   -> Font: ${ollamaPath}`);
            console.log("   -> Aix√≤ pot trigar uns minuts (llegint 2GB+)... NO APAGUIS.");

            // PROVEM AMB 'from' DIRECTAMENT (Segons error "neither from or files specified")
            const payload = {
                name: MODEL_NAME,
                modelfile: `FROM ${ollamaPath}`, // Mantenim per si de cas
                from: ollamaPath,                // AFEGIT: La clau que demana error
                stream: false
            };

            console.log("   -> Payload:", JSON.stringify(payload));

            const createRes = await fetch(`${ollamaHost}/api/create`, {
                method: "POST",
                headers: { "Content-Type": "application/json" },
                body: JSON.stringify(payload)
            });

            if (!createRes.ok) {
                const errText = await createRes.text();
                console.error(`‚ö†Ô∏è [aiService] Error creant model (${createRes.status}): ${errText}`);
                console.log("   -> Reintentant en 5 segons...");
                await new Promise(r => setTimeout(r, 5000));
                continue;
            }

            console.log(`üéâ [aiService] MODEL '${MODEL_NAME}' CREAT AMB √àXIT!`);
            break; // √àxit total

        } catch (error) {
            console.error(`‚ùå [aiService] Error inesperat en la inicialitzaci√≥: ${error.message}`);
            await new Promise(r => setTimeout(r, 5000));
        }
    }

    // C. WARM UP (Opcional)
    console.log("üî• [aiService] Escalfant motor d'infer√®ncia...");
    try {
        await openai.chat.completions.create({
            model: MODEL_NAME,
            messages: [{ role: "user", content: "hi" }],
            max_tokens: 1
        });
        console.log("üü¢ [aiService] SISTEMA LLEST I OPERATIU.");
    } catch (e) {
        console.warn("‚ö†Ô∏è [aiService] Warm-up sense resposta (normal si est√† carregant lazy): " + e.message);
    }

    return true;
}

// Mantenim compatibilitat amb server.js
const checkConnection = initializeAI;


/**
 * Genera un resum utilitzant IA LOCAL (sense streaming HTTP directe).
 * Retorna el text complet quan acaba.
 * @param {string} role - 'docent' o 'orientador'
 * @param {function} onProgress - Callback opcional (textParcial, percentatge)
 */
async function generateSummaryLocal(text, role, onProgress) {

    // OPTIMITZACI√ì EXTREMA: Neteja de "soroll"
    let cleanText = text
        .replace(/Cop√®rnic, 84 08006 Barcelona/g, '')
        .replace(/tel√®fon: 93 200 49 13/g, '')
        .replace(/fax: 93 414 04 34/g, '')
        .replace(/institutmontserrat@xtec.cat/g, '')
        .replace(/www.institutmontserrat.cat/g, '')
        .replace(/Pla individualitzat: xxxx xxx xxx/g, '')
        .replace(/S√≠ No/g, '') // Eliminar cap√ßaleres de taula
        .replace(/Adaptacions que es proposen/g, '')
        .replace(/\n\s*\n/g, '\n');

    const limit = role === 'global' ? 6000 : 3500;
    const MAX_CHARS = limit;
    const truncatedText = cleanText.length > MAX_CHARS ? cleanText.substring(0, MAX_CHARS) + "..." : cleanText;

    let systemPrompt = "Ets un expert en educaci√≥. La teva tasca √©s extreure informaci√≥ i formatar-la.";
    let userPrompt = "";

    if (role === 'orientador') {
        userPrompt = `Analitza el seg√ºent text d'un Pla Individualitzat (PI) i genera un resum estructurat.
        
        <TEXT_PI>
        ${truncatedText}
        </TEXT_PI>

        INSTRUCCIONS DE FORMAT (SEGUEIX-LES AL PEU DE LA LLETRA):
        
        Vull que generis EXACTAMENT aquestes 5 seccions. No inventis res. Si no trobes informaci√≥, digues "No especificat".

        1. PERFIL DE L'ALUMNE
        (Escriu un par√†graf breu de 2-3 l√≠nies sobre el curs i problemes generals. NO facis llistes.)

        2. DIAGN√íSTIC
        (Escriu un par√†graf breu de 2-3 l√≠nies amb el diagn√≤stic concret. NO facis llistes.)

        3. JUSTIFICACI√ì DEL PI
        (Breu explicaci√≥ textual.)

        4. ORIENTACI√ì A L'AULA
        (Fes una llista amb guions '-' de pautes per al professor. Elimina les 'X' finals.)

        5. MAT√àRIES
        (Si veus mat√®ries espec√≠fiques com Mates/Catal√† amb 'X', llista-les: "- Mat√®ria: Adaptaci√≥". Si nom√©s hi ha adaptacions generals, escriu un par√†graf explicatiu.)

        IMPORTANT: Comen√ßa directament amb "1. PERFIL DE L'ALUMNE".`;

    } else {
        // DOCENT
        userPrompt = `Analitza el seg√ºent document (Pla Individualitzat) i extreu-ne les adaptacions.
        
        <TEXT_PI>
        ${truncatedText}
        </TEXT_PI>

        INSTRUCCIONS DE GENERACI√ì (Imprescindible seguir l'estructura):

        1. PERFIL DE L'ALUMNE
        (Resum de 2-3 l√≠nies en forma de text seguit. Curs i dificultats globals.)

        2. DIAGN√íSTIC
        (Resum de 2-3 l√≠nies en forma de text seguit. Problema espec√≠fic.)

        3. ORIENTACI√ì A L'AULA
        (Llista de punts amb guions '-'. Ex: "- Donar m√©s temps". Neteja les 'X'.)

        4. MAT√àRIES
        (ATENCI√ì: Busca a la taula. Si "Matem√†tiques" t√© una 'X', posa: "- Matem√†tiques: [Adaptaci√≥]". Si √©s "Totes les mat√®ries", fes un par√†graf explicant-ho.)

        5. CRITERIS D'AVALUACI√ì
        (Llista o text segons el cas.)

        RESPOSTA:`;
    }

    const messages = [
        { role: "system", content: systemPrompt },
        { role: "user", content: userPrompt }
    ];

    try {
        console.log(`ü§ñ [aiService] Enviant petici√≥ a IA Local...`);
        const completion = await openai.chat.completions.create({
            model: MODEL_NAME,
            messages: messages,
            temperature: 0.1,
            max_tokens: 2000,
            stream: true,
            top_p: 0.9,
            presence_penalty: 0.6,
            frequency_penalty: 1.1
        });

        console.log("ü§ñ [aiService] Connexi√≥ establerta amb LLM! Esperant el primer token (Fase de Lectura/Pre-fill)...");

        let fullText = "";
        // Seccions esperades per calcular el progr√©s (aprox 20% per secci√≥)
        // MODIFICAT: Keywords actualitzades segons els nous prompts (Docent/Orientador)
        const sections = ["PERFIL", "DADES", "DIAGN√íSTIC", "ORIENTACI√ì", "ADAPTACIONS", "MAT√àRIES", "ASSIGNATURES", "CRITERIS", "JUSTIFICACI√ì"];
        let isFirst = true;
        let chunkCount = 0;

        for await (const chunk of completion) {
            // FASE 2: ESCRIPTURA (Reset a 0% -> 100%)
            if (isFirst) {
                console.log("ü§ñ [aiService] Primer token rebut! Comen√ßa la generaci√≥ de text.");
                isFirst = false;
                currentProgress = 0; // Reiniciem la barra per a la fase d'escriptura
            }

            chunkCount++;
            const content = chunk.choices[0]?.delta?.content || "";

            // Log de "batec" cada 10 chunks per veure que est√† viu a la terminal (M√©s freq√ºent)
            if (chunkCount % 10 === 0) {
                console.log(`... generant (${chunkCount} tokens)`); // M√©s visible als logs de Docker
            }

            fullText += content;

            if (onProgress) {
                // C√†lcul simple de progr√©s: Quantes seccions hem trobat ja?
                let foundCount = 0;
                sections.forEach(s => {
                    if (fullText.includes(s)) foundCount++;
                });

                // C√†lcul de progr√©s d'escriptura (0 a 100)
                // MODIFICAT: Ajustem a 2000 tokens (resum complet)
                // (chunkCount / 20) -> 2000 tokens = 100%
                const chunkProgress = (chunkCount / 20);
                const sectionProgress = foundCount * 5; // M√©s pes a les seccions per compensar

                let writeProgress = chunkProgress + sectionProgress;

                // Enviem text ple -> Servidor marca "GENERANT..."
                // AWAIT IMPORTANT: Esperem que s'actualitzi la BD abans de continuar per evitar race conditions al final
                await onProgress(fullText, Math.min(Math.floor(writeProgress), 99));
            }
        }

        console.log(`ü§ñ [IA Local] Generaci√≥ finalitzada amb √®xit. Longitud: ${fullText.length} car√†cters.`);
        return fullText;
    } catch (error) {
        console.error("‚ùå Error IA Local:", error);
        throw new Error("Error connectant amb el contenidor d'IA Local.");
    }
}

/**
 * Xat r√†pid amb el document.
 * @param {string} text - Text del document
 * @param {string} question - Pregunta de l'usuari
 */
async function chatWithDocument(text, question) {
    // OPTIMITZACI√ì EXTREMA: 1200 chars per velocitat m√†xima al xat
    const MAX_CHARS = 1200;
    const truncatedText = text.length > MAX_CHARS ? text.substring(0, MAX_CHARS) + "..." : text;

    const messages = [
        {
            role: "system",
            content: `Ets un motor de cerca sem√†ntic.
            TASCA: Interpretar qu√® vol l'usuari i trobar la frase LITERAL del text que ho respon, encara que no faci servir les mateixes paraules.
            
            EXEMPLES:
            - "comportament" -> Busca frases sobre "conducta", "actitud", "normes".
            - "qu√® t√©?" -> Busca "diagn√≤stic", "trastorn", "dificultats".
            
            RESPOSTA: Retorna NOM√âS el fragment de text exacte del document. Si no ho trobes, digues NO_TROBAT.`
        },
        {
            role: "user",
            content: `DOCUMENT:\n"${truncatedText}"\n\nPREGUNTA: "${question}"\n\nRESPOSTA LITERAL DEL DOCUMENT:`
        }
    ];

    try {
        const completion = await openai.chat.completions.create({
            model: MODEL_NAME,
            messages: messages,
            temperature: 0.0, // Determinista (sempre la mateixa resposta)
            max_tokens: 60, // Molt curt (nom√©s volem la frase)
            stream: false
        });
        return completion.choices[0].message.content;
    } catch (error) {
        console.error("‚ùå Error Chat IA:", error);
        throw new Error("Error connectant amb la IA.");
    }
}

module.exports = { generateSummaryLocal, checkConnection, chatWithDocument };