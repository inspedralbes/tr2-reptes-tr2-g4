const OpenAI = require("openai");

// Configuraci√≥ del client per a IA LOCAL
const openai = new OpenAI({
  baseURL: "http://pi_llm:8080/v1", // MODIFICAT: Connecta amb 'pi_llm' (nom real del contenidor)
  apiKey: "sk-no-key-required",  // La IA local no necessita clau real
  timeout: 30 * 60 * 1000,       // NOU: 30 minuts de timeout (augmentat per si va lent)
});

/**
 * Comprova si el contenidor de la IA est√† disponible.
 * Ho intenta 5 vegades abans de rendir-se.
 */
async function checkConnection(retries = 100) {
  const url = "http://pi_llm:8080/health"; // Endpoint de salut de llama.cpp
  console.log(`üîç [aiService] Comprovant connexi√≥ amb IA Local (${url})...`);

  for (let i = 0; i < retries; i++) {
    try {
      const response = await fetch(url);
      if (response.ok) {
        console.log("‚úÖ [aiService] IA Local ONLINE (Port 8080 obert).");

        // NOU: Test real de generaci√≥ per confirmar que "pensa"
        console.log("üß™ [aiService] Fent prova de generaci√≥ r√†pida (Warm-up)...");
        try {
          await openai.chat.completions.create({
            model: "default-model",
            messages: [{ role: "user", content: "Test" }],
            max_tokens: 1
          });
          console.log("üöÄ [aiService] TEST SUPERAT! La IA est√† generant text correctament.");
        } catch (e) {
          console.warn("‚ö†Ô∏è [aiService] El test de generaci√≥ ha fallat (potser est√† carregant model):", e.message);
        }

        return true;
      }
      // Si respon per√≤ no √©s OK (ex: 503 Loading...), avisem
      console.warn(`‚ö†Ô∏è [aiService] La IA est√† carregant models (Status: ${response.status})...`);
    } catch (error) {
      console.warn(`‚ö†Ô∏è [aiService] Intent ${i + 1}/${retries} fallit: ${error.message}`);
    }
    // MOGUT: Esperem 3s SEMPRE si no hem acabat, tant si falla la xarxa com si est√† carregant
    if (i < retries - 1) await new Promise(r => setTimeout(r, 3000));
  }
  console.error("‚ùå [aiService] IMPOSSIBLE CONNECTAR AMB LA IA. Revisa que el contenidor 'pi_llm' estigui enc√®s i a la mateixa xarxa.");
  return false;
}

/**
 * Genera un resum utilitzant IA LOCAL (sense streaming HTTP directe).
 * Retorna el text complet quan acaba.
 * @param {string} role - 'docent' o 'orientador'
 * @param {function} onProgress - Callback opcional (textParcial, percentatge)
 */
async function generateSummaryLocal(text, role, onProgress) {

  // FASE 0: ESPERAR A QUE LA IA ESTE LLESTA (Status 200)
  // Si el servidor retorna 503, significa que encara est√† carregant el model. Esperem.
  const healthUrl = "http://pi_llm:8080/health";
  let ready = false;
  let attempts = 0;
  while (!ready && attempts < 20) { // Esperem fins a 1 minut extra (20 * 3s)
    try {
      const hRes = await fetch(healthUrl);
      if (hRes.ok) {
        ready = true;
      } else {
        console.log(`‚è≥ [aiService] La IA encara est√† carregant (Status ${hRes.status}). Esperant 3s...`);
        await new Promise(r => setTimeout(r, 3000));
      }
    } catch (e) {
      console.log(`‚è≥ [aiService] Esperant que el contenidor IA respongui...`);
      await new Promise(r => setTimeout(r, 3000));
    }
    attempts++;
  }

  // Retallem el text per no saturar el context del model
  // Si √©s un resum global, permetem molt m√©s context per encabir diversos documents (aprox 10.000 tokens)
  const limit = role === 'global' ? 40000 : 25000;
  const MAX_CHARS = limit;
  const truncatedText = text.length > MAX_CHARS ? text.substring(0, MAX_CHARS) + "..." : text;

  let currentProgress = 0;

  // FASE 1: LECTURA
  // Eliminem la simulaci√≥. No enviarem progr√©s fals. El frontend mostrar√† "Llegint..." sense barra o amb barra indeterminada.

  // --- SELECCI√ì DE PROMPT SEGONS ROL ---
  let systemPrompt = "";

  if (role === 'global') {
    // PROMPT PER A RESUM GLOBAL (Historial)
    systemPrompt = `Ets un assistent expert en educaci√≥ especialITZADA.
      OBJECTIU: Generar un resum global, profund i cronol√≤gic de l'evoluci√≥ de l'alumne.
      
      ESTRUCTURA OBLIGAT√íRIA:
      1. **EVOLUCI√ì**: Progr√©s des del primer document. Detecta canvis de centre o de suport (SIEI, ordin√†ria).
      2. **PUNTS CLAU RECURRENTS**: Diagn√≤stics t√®cnics (Paresia, TDAH, Disl√®xia) i barreres.
      3. **ADAPTACIONS CONSTANTS**: Mesures de suport que persisteixen (Auxiliars, Fisioter√†pia).
      4. **ESTAT ACTUAL**: Prioritats de l'√∫ltim curs (3r ESO, 4t ESO, etc.).

      FORMAT: Text professional i directe.`;
  } else if (role === 'orientador') {
    // PROMPT PER A ORIENTADORS
    systemPrompt = `Ets un assistent expert per a orientadors educatius.
      OBJECTIU: Extraure informaci√≥ t√®cnica i jur√≠dica del PI.
      
      ESTRUCTURA OBLIGAT√íRIA:
      1. PERFIL DE L'ALUMNE: Descripci√≥ biopsicosocial i fets rellevants (adoptat, nouvingut, etc.).
      2. DIAGN√íSTIC I NECESSITATS: Diagn√≤stic literal (ej: Tetrapar√®sia esp√†stica, Disl√®xia severa). Menciona el grau de discapacitat (CAD %) si apareix.
      3. JUSTIFICACI√ì DEL PI: Motiu de l'elaboraci√≥ (Dictamen, NESE, etc.).
      4. MESURES I SUPORTS: Professionals que intervenen (SIEI, EAP, Fisioterapeuta, Auxiliar). Menciona l'equipament (Tobii, Braille, Tablet).
      5. SEGUIMENT PER MAT√àRIES: Llistat d'assignatures i nivell d'assoliment.

      FORMAT: "Idea clau. [[Detall: Text literal...]]"`;
  } else {
    // PROMPT PER A DOCENTS (Defecte)
    systemPrompt = `Ets un assistent expert per a professors d'aula.
      OBJECTIU: Guia pr√†ctica per saber com treballar amb l'alumne.
      
      ESTRUCTURA OBLIGAT√íRIA:
      1. PERFIL DE L'ALUMNE: Com apr√®n i quin car√†cter t√© (autoexigent, participatiu, t√≠mid).
      2. DIAGN√íSTIC: Resum entenedor del diagn√≤stic i el curs actual.
      3. ORIENTACI√ì A L'AULA: Metodologia concreta (Tobii-Eye Tracking, ordinador, Braille, m√©s temps, enunciats curts).
      4. ASSIGNATURES I MAT√àRIES: Llistat exhaustiu de cada mat√®ria detectada al document amb les seves adaptacions.
      5. CRITERIS D'AVALUACI√ì: Molt important: com s'ha de qualificar (ej: no penalitzar faltes, valorar contingut sobre forma, √∫s de calculadora).

      FORMAT: "Resum executiu. [[Detall: Cita literal del document...]]"`;
  }

  const messages = [
    {
      role: "system",
      content: `${systemPrompt}
      
      INSTRUCCIONS CR√çTIQUES DE FORMAT I CONTINGUT:
      1. T√çTOLS OBLIGATORIS: Genera SEMPRE les seccions exactes.
      2. ANONIMITZACI√ì: Substitueix el nom de l'alumne per "L'alumne/a".
      3. DETECCI√ì DE MAT√àRIES (MOLT IMPORTANT): El document sol tenir taules amb assignatures (Catal√†, Castell√†, Matem√†tiques, Angl√®s, etc.). Has de llistar-les ABSOLUTAMENT TOTES. No te'n deixis cap.
      4. DETALLS LITERALS: Dins de [[Detall: ...]] has de posar el contingut literal, especialment en ASSIGNATURES i CRITERIS D'AVALUACI√ì. Si el document diu "Adapaci√≥ de continguts: ...", copia-ho tot.
      5. EVITA RESUMS GEN√àRICS: Si el document diu coses espec√≠fiques de Matem√†tiques, no digues "adaptacions en general", digues exactament qu√® es fa en Matem√†tiques.
      6. BUSCA EL CURS: Identifica a quin curs pertany el document (1r ESO, 2n Prim√†ria, etc.) i menciona'l al perfil.
      7. NO INVENTIS: Si una secci√≥ no t√© informaci√≥ al text, simplement no la posis o digues "Informaci√≥ no disponible al document".
      8. NO ASTERISCS: No usis asteriscs (*) ni guions (-) per llistes, usa par√†grafs o salts de l√≠nia nets.
      
      Analitza el seg√ºent text amb m√†xima atenci√≥ als detalls acad√®mics:`
    },
    {
      role: "user",
      content: `DOCUMENT PI:\n\n${truncatedText}`
    }
  ];

  try {
    console.log(`ü§ñ [aiService] Enviant petici√≥ a IA Local (http://pi_llm:8080/v1)...`);
    const completion = await openai.chat.completions.create({
      model: "default-model",
      messages: messages,
      temperature: 0.1,
      max_tokens: 4000, // AUGMENTAT: 4000 tokens per permetre resums molt detallats sense talls
      stream: true,
      top_p: 0.9,
      presence_penalty: 0,
      frequency_penalty: 0
    });

    console.log("ü§ñ [aiService] Connexi√≥ establerta amb LLM! Esperant el primer token (Fase de Lectura/Pre-fill)...");

    let fullText = "";
    // Seccions esperades per calcular el progr√©s (aprox 20% per secci√≥)
    // MODIFICAT: Keywords actualitzades segons els nous prompts (Docent/Orientador)
    const sections = ["PERFIL", "DADES", "DIAGN√íSTIC", "ORIENTACI√ì", "ADAPTACIONS", "MAT√àRIES", "ASSIGNATURES", "CRITERIS", "JUSTIFICACI√ì"];
    let isFirst = true;
    let chunkCount = 0;

    for await (const chunk of completion) {
      // FASE 2: ESCRIPTURA (Reset a 0% -> 100%)
      if (isFirst) {
        console.log("ü§ñ [aiService] Primer token rebut! Comen√ßa la generaci√≥ de text.");
        isFirst = false;
        currentProgress = 0; // Reiniciem la barra per a la fase d'escriptura
      }

      chunkCount++;
      const content = chunk.choices[0]?.delta?.content || "";

      // Log de "batec" cada 10 chunks per veure que est√† viu a la terminal (M√©s freq√ºent)
      if (chunkCount % 10 === 0) {
        console.log(`... generant (${chunkCount} tokens)`); // M√©s visible als logs de Docker
      }

      fullText += content;

      if (onProgress) {
        // C√†lcul simple de progr√©s: Quantes seccions hem trobat ja?
        let foundCount = 0;
        sections.forEach(s => {
          if (fullText.includes(s)) foundCount++;
        });

        // C√†lcul de progr√©s d'escriptura (0 a 100)
        // MODIFICAT: Ajustem a 2000 tokens (resum complet)
        // (chunkCount / 20) -> 2000 tokens = 100%
        const chunkProgress = (chunkCount / 20);
        const sectionProgress = foundCount * 5; // M√©s pes a les seccions per compensar

        let writeProgress = chunkProgress + sectionProgress;

        // Enviem text ple -> Servidor marca "GENERANT..."
        // AWAIT IMPORTANT: Esperem que s'actualitzi la BD abans de continuar per evitar race conditions al final
        await onProgress(fullText, Math.min(Math.floor(writeProgress), 99));
      }
    }

    console.log(`ü§ñ [IA Local] Generaci√≥ finalitzada amb √®xit. Longitud: ${fullText.length} car√†cters.`);
    return fullText;
  } catch (error) {
    console.error("‚ùå Error IA Local:", error);
    throw new Error("Error connectant amb el contenidor d'IA Local.");
  }
}

/**
 * Xat r√†pid amb el document.
 * @param {string} text - Text del document
 * @param {string} question - Pregunta de l'usuari
 */
async function chatWithDocument(text, question) {
  // OPTIMITZACI√ì EXTREMA: 1200 chars per velocitat m√†xima al xat
  const MAX_CHARS = 1200;
  const truncatedText = text.length > MAX_CHARS ? text.substring(0, MAX_CHARS) + "..." : text;

  const messages = [
    {
      role: "system",
      content: `Ets un motor de cerca sem√†ntic.
            TASCA: Interpretar qu√® vol l'usuari i trobar la frase LITERAL del text que ho respon, encara que no faci servir les mateixes paraules.
            
            EXEMPLES:
            - "comportament" -> Busca frases sobre "conducta", "actitud", "normes".
            - "qu√® t√©?" -> Busca "diagn√≤stic", "trastorn", "dificultats".
            
            RESPOSTA: Retorna NOM√âS el fragment de text exacte del document. Si no ho trobes, digues NO_TROBAT.`
    },
    {
      role: "user",
      content: `DOCUMENT:\n"${truncatedText}"\n\nPREGUNTA: "${question}"\n\nRESPOSTA LITERAL DEL DOCUMENT:`
    }
  ];

  try {
    const completion = await openai.chat.completions.create({
      model: "default-model",
      messages: messages,
      temperature: 0.0, // Determinista (sempre la mateixa resposta)
      max_tokens: 60, // Molt curt (nom√©s volem la frase)
      stream: false
    });
    return completion.choices[0].message.content;
  } catch (error) {
    console.error("‚ùå Error Chat IA:", error);
    throw new Error("Error connectant amb la IA.");
  }
}

module.exports = { generateSummaryLocal, checkConnection, chatWithDocument };