const OpenAI = require("openai");

// Configuraci√≥ del client per a IA LOCAL
const openai = new OpenAI({
  baseURL: "http://llm:8080/v1", // Connecta amb el contenidor 'llm' del docker-compose
  apiKey: "sk-no-key-required",  // La IA local no necessita clau real
});

/**
 * Genera un resum utilitzant IA LOCAL (sense streaming HTTP directe).
 * Retorna el text complet quan acaba.
 * @param {function} onProgress - Callback opcional (textParcial, percentatge)
 */
async function generateSummaryLocal(text, onProgress) {

  // Retallem el text per no saturar el context del model
  const MAX_CHARS = 6000; // Redu√Øt encara m√©s per garantir resposta r√†pida en CPU
  const truncatedText = text.length > MAX_CHARS ? text.substring(0, MAX_CHARS) + "..." : text;

  // --- C√ÄLCUL DE PROGR√âS DE LECTURA (REALISTA) ---
  // Estimaci√≥: 4 car√†cters ~= 1 token.
  const estimatedTokens = truncatedText.length / 4;
  // Velocitat conservadora en CPU: ~40 tokens/segon per processar el prompt
  const processingSpeed = 40; 
  const estimatedDurationSecs = Math.max(5, estimatedTokens / processingSpeed);
  
  let currentProgress = 0;
  let readingInterval = null;

  // FASE 1: LECTURA (0% -> 99%)
  // La barra s'omple completament mentre la IA "llegeix" el document
  readingInterval = setInterval(() => {
      const increment = 100 / estimatedDurationSecs; 
      currentProgress += increment;
      if (currentProgress > 99) currentProgress = 99; // No passar de 99 fins que acabi
      
      // Enviem text buit -> Servidor marca "LLEGINT..."
      if (onProgress) onProgress("", Math.floor(currentProgress));
  }, 1000);

  const messages = [
    {
      role: "system",
      content: `Ets un analista documental expert. La teva missi√≥ √©s facilitar el trasp√†s d'informaci√≥ d'alumnes que passen de Secund√†ria a Formaci√≥ Professional (FP).
      
      OBJECTIU PRINCIPAL: El nou centre ha de rebre informaci√≥ clara sobre:
      1. Les adaptacions educatives aplicades.
      2. Quines han funcionat i en quin context.
      3. Quines es podrien aplicar o adaptar al nou centre (FP).

      ESTRUCTURA OBLIGAT√íRIA: Has de generar CINC seccions. Cada secci√≥ ha de comen√ßar AMB EL T√çTOL EXACTE en una l√≠nia separada, sense text addicional en aquella l√≠nia. Els t√≠tols s√≥n:
      1. PERFIL DE L'ALUMNE
      2. DIFICULTATS I BARRERES
      3. ADAPTACIONS METODOL√íGIQUES
      4. AVALUACI√ì I QUALIFICACI√ì
      5. RECOMANACIONS I TRASP√ÄS

      REGLA D'OR DEL FORMAT "DETALL":
      Per a cada punt, has de seguir aquest format: "Idea principal resumida. [[Detall: **[Font: Secci√≥]** Copia aqu√≠ el text original complet del PDF per si el docent necessita m√©s context o informaci√≥ extra.]]".
      NO resumeixis en exc√©s, extreu les frases clau literals.

      INSTRUCCIONS ESPEC√çFIQUES:
      - **Perfil**: Resum breu (2-3 l√≠nies) amb dades acad√®miques, diagn√≤stic i motiu.
      - **Adaptacions per Mat√®ries**: 
        - FORMAT: Fes una llista on cada punt comenci amb l'assignatura o √†mbit seguit de dos punts.
        - Exemple: "- Matem√†tiques: √ös de calculadora..."
        - NO facis taules Markdown. Utilitza llistes per aprofitar millor l'espai en columnes.
      - **Recomanacions**: Redacta un text fluid per√≤ MOLT ESPEC√çFIC. NO facis servir frases gen√®riques com "continuar amb les adaptacions". Has d'explicar QUINES s√≥n (ex: "Donar m√©s temps", "√ös de calculadora", "Pautes escrites").
      - **Exhaustivitat**: Processa totes les p√†gines.
      - **Taules Originals**: Si detectes taules amb 'X' al PDF, indica clarament qu√® est√† marcat dins del detall.
      - **Noms**: Ignora noms de professionals.

      Exemple de sortida desitjada:
      - Matem√†tiques: √ös de calculadora. [[Detall: **[Font: Adaptacions]** L'alumne millora amb calculadora...]]

      Processa tot el text proporcionat.`
    },
    {
      role: "user",
      content: `Analitza aquest PI i extreu-ne la informaci√≥ rellevant:\n\n${truncatedText}`
    }
  ];

  try {
    console.log("ü§ñ [aiService] Enviant petici√≥ a IA Local...");
    const completion = await openai.chat.completions.create({
      model: "default-model", // El nom √©s indiferent per a llama.cpp
      messages: messages,
      temperature: 0.1,
      stream: true, // ACTIVEM STREAMING per veure el progr√©s
    });

    let fullText = "";
    // Seccions esperades per calcular el progr√©s (aprox 20% per secci√≥)
    const sections = ["PERFIL", "DIFICULTATS", "ADAPTACIONS", "AVALUACI√ì", "RECOMANACIONS"];
    let isFirst = true;
    let chunkCount = 0;

    for await (const chunk of completion) {
        // FASE 2: ESCRIPTURA (Reset a 0% -> 100%)
        if (isFirst) {
            clearInterval(readingInterval);
            console.log("ü§ñ [aiService] Primer token rebut! Comen√ßa la generaci√≥ de text.");
            isFirst = false;
            currentProgress = 0; // Reiniciem la barra per a la fase d'escriptura
        }

        chunkCount++;
        const content = chunk.choices[0]?.delta?.content || "";
        
        // Log de "batec" cada 50 chunks per veure que est√† viu a la terminal
        if (chunkCount % 50 === 0) {
            console.log(`... generant (${chunkCount} tokens)`); // M√©s visible als logs de Docker
        }

        fullText += content;

        if (onProgress) {
            // C√†lcul simple de progr√©s: Quantes seccions hem trobat ja?
            let foundCount = 0;
            sections.forEach(s => {
                if (fullText.includes(s)) foundCount++;
            });
            
            // C√†lcul de progr√©s d'escriptura (0 a 100)
            // 1000 tokens aprox per un resum complet -> 100%
            const chunkProgress = Math.min(chunkCount / 10, 80); 
            const sectionProgress = foundCount * 4;
            
            let writeProgress = chunkProgress + sectionProgress;
            
            // Enviem text ple -> Servidor marca "GENERANT..."
            onProgress(fullText, Math.min(Math.floor(writeProgress), 99));
        }
    }

    console.log(`ü§ñ [IA Local] Generaci√≥ finalitzada amb √®xit. Longitud: ${fullText.length} car√†cters.`);
    return fullText;
  } catch (error) {
    if (readingInterval) clearInterval(readingInterval);
    console.error("‚ùå Error IA Local:", error);
    throw new Error("Error connectant amb el contenidor d'IA Local.");
  }
}

module.exports = { generateSummaryLocal };