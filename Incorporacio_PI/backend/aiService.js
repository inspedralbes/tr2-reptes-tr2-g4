const OpenAI = require("openai");

// Configuraci√≥ del client per a OLLAMA
const openai = new OpenAI({
  baseURL: "http://pi_llm:11434/v1", // Port d'Ollama
  apiKey: "ollama",  // Ollama requereix una string qualsevol
  timeout: 60 * 60 * 1000, // 1 hora timeout
});

const MODEL_NAME = "pi-model"; // Nom intern que donarem al teu model dins d'Ollama

/**
 * Assegura que el model estigui creat dins d'Ollama a partir del fitxer .gguf
 */
async function ensureModelExists() {
    const modelPath = "/models/Llama-3.2-3B-Instruct-Q4_K_M.gguf";
    try {
        // 1. Check if model exists
        const listRes = await fetch("http://pi_llm:11434/api/tags");
        if (listRes.ok) {
            const data = await listRes.json();
            if (data.models.some(m => m.name === MODEL_NAME || m.name === `${MODEL_NAME}:latest`)) {
                console.log(`‚úÖ [aiService] Model '${MODEL_NAME}' ja est√† llest a Ollama.`);
                return;
            }
        }
        
        // 2. Create model
        console.log(`‚öôÔ∏è [aiService] Important model '${MODEL_NAME}' a Ollama...`);
        const createRes = await fetch("http://pi_llm:11434/api/create", {
            method: "POST",
            headers: { "Content-Type": "application/json" },
            body: JSON.stringify({
                name: MODEL_NAME,
                modelfile: `FROM ${modelPath}`
            })
        });

        if (!createRes.ok) throw new Error(`Error creant model: ${createRes.statusText}`);
        
        // Consume stream to wait for completion
        const reader = createRes.body.getReader();
        while (true) {
            const { done } = await reader.read();
            if (done) break;
        }
        console.log(`‚úÖ [aiService] Model '${MODEL_NAME}' creat correctament.`);
        
    } catch (e) {
        console.error("‚ùå [aiService] Error configurant model Ollama:", e);
    }
}

/**
 * Comprova si el contenidor de la IA est√† disponible.
 * Ho intenta 5 vegades abans de rendir-se.
 */
async function checkConnection(retries = 100) {
    const url = "http://pi_llm:11434/api/version"; // Endpoint salut Ollama
    console.log(`üîç [aiService] Comprovant connexi√≥ amb Ollama (${url})...`);
    
    for (let i = 0; i < retries; i++) {
        try {
            const response = await fetch(url);
            if (response.ok) {
                console.log("‚úÖ [aiService] Ollama ONLINE.");
                
                // 2. Comprovem/Creem el model autom√†ticament des del .gguf
                await ensureModelExists();
                
                // NOU: Test real de generaci√≥ per confirmar que "pensa"
                console.log("üß™ [aiService] Fent prova de generaci√≥ r√†pida (Warm-up)...");
                try {
                    await openai.chat.completions.create({
                        model: MODEL_NAME,
                        messages: [{ role: "user", content: "Test" }],
                        max_tokens: 1
                    });
                    console.log("üöÄ [aiService] TEST SUPERAT! La IA est√† generant text correctament.");
                } catch (e) {
                    console.warn("‚ö†Ô∏è [aiService] El test de generaci√≥ ha fallat (potser est√† carregant model):", e.message);
                }

                return true;
            }
            // Si respon per√≤ no √©s OK (ex: 503 Loading...), avisem
            console.warn(`‚ö†Ô∏è [aiService] La IA est√† carregant models (Status: ${response.status})...`);
        } catch (error) {
            console.warn(`‚ö†Ô∏è [aiService] Intent ${i+1}/${retries} fallit: ${error.message}`);
        }
        // MOGUT: Esperem 3s SEMPRE si no hem acabat, tant si falla la xarxa com si est√† carregant
        if (i < retries - 1) await new Promise(r => setTimeout(r, 3000));
    }
    console.error("‚ùå [aiService] IMPOSSIBLE CONNECTAR AMB LA IA. Revisa que el contenidor 'pi_llm' estigui enc√®s i a la mateixa xarxa.");
    return false;
}

/**
 * Genera un resum utilitzant IA LOCAL (sense streaming HTTP directe).
 * Retorna el text complet quan acaba.
 * @param {string} role - 'docent' o 'orientador'
 * @param {function} onProgress - Callback opcional (textParcial, percentatge)
 */
async function generateSummaryLocal(text, role, onProgress) {

  // Retallem el text per no saturar el context del model
  // Si √©s un resum global, permetem m√©s context per encabir diversos documents
  const limit = role === 'global' ? 10000 : 8000; // REDU√èT: Optimitzaci√≥ de velocitat (CPU)
  const MAX_CHARS = limit;
  const truncatedText = text.length > MAX_CHARS ? text.substring(0, MAX_CHARS) + "..." : text;

  let currentProgress = 0;
  
  // FASE 1: LECTURA
  // Eliminem la simulaci√≥. No enviarem progr√©s fals. El frontend mostrar√† "Llegint..." sense barra o amb barra indeterminada.

  // --- SELECCI√ì DE PROMPT SEGONS ROL ---
  let systemPrompt = "";
  
  if (role === 'global') {
    // PROMPT PER A RESUM GLOBAL (Historial)
    systemPrompt = `Ets un assistent expert en educaci√≥.
      OBJECTIU: Generar un resum global i cronol√≤gic de l'evoluci√≥ de l'alumne basant-se en tots els seus Plans Individualitzats (PI).
      
      ESTRUCTURA OBLIGAT√íRIA (Usa exactament aquests t√≠tols en maj√∫scules i negreta):
      1. **EVOLUCI√ì**: Breu descripci√≥ del progr√©s. IMPORTANT: No inventis el curs actual. Si el document no ho diu clarament, digues "Curs no especificat".
      2. **PUNTS CLAU RECURRENTS**: Diagn√≤stics o dificultats que es repeteixen.
      3. **ADAPTACIONS CONSTANTS**: Mesures mantingudes en el temps.
      4. **ESTAT ACTUAL**: Situaci√≥ segons l'√öLTIM document (per data o context). Sigues prec√≠s amb el curs i les necessitats actuals.

      FORMAT: Text net. No posis t√≠tol general "HISTORIAL...".`;
  } else if (role === 'orientador') {
    // PROMPT PER A ORIENTADORS
    systemPrompt = `Ets un assistent expert per a orientadors educatius.
      OBJECTIU: Extreure informaci√≥ clau per a l'orientaci√≥ i seguiment de l'alumne.
      
      ESTRUCTURA OBLIGAT√íRIA (5 SECCIONS):
      1. PERFIL DE L'ALUMNE (Text seguit en un sol par√†graf. NO llistes.)
      2. DIAGN√íSTIC (Text seguit en un sol par√†graf, incloent observacions. NO llistes.)
      3. JUSTIFICACI√ì DEL PI (Text seguit explicant el motiu basat en el diagn√≤stic. NO llistes.)
      4. ORIENTACI√ì A L'AULA (Pautes d'actuaci√≥. NO incloguis dades administratives finals.)
      5. MAT√àRIES (Adaptacions curriculars i Avaluaci√≥)

      FORMAT GENERAL: "Idea clau molt breu. [[Detall: Text original...]]"
      FORMAT MAT√àRIES: "Nom Mat√®ria: Resum molt breu. [[Detall: Contingut complet i Criteris d'Avaluaci√≥ originals del document]]"`;
  } else {
    // PROMPT PER A DOCENTS (Defecte)
    systemPrompt = `Ets un assistent expert per a docents.
      OBJECTIU: Facilitar informaci√≥ pr√†ctica per a l'aula i l'avaluaci√≥.
      
      ESTRUCTURA OBLIGAT√íRIA (5 SECCIONS):
      1. PERFIL DE L'ALUMNE (Text seguit en un sol par√†graf. NO llistes.)
      2. DIAGN√íSTIC (Text seguit en un sol par√†graf, incloent observacions. NO llistes.)
      3. ORIENTACI√ì A L'AULA (Pautes d'actuaci√≥. NO incloguis dades administratives finals.)
      4. ASSIGNATURES (Adaptacions espec√≠fiques per mat√®ria)
      5. CRITERIS D'AVALUACI√ì (Com avaluar)

      FORMAT GENERAL: "Idea clau molt breu. [[Detall: Text original...]]"
      FORMAT ASSIGNATURES: "Nom Mat√®ria: Resum molt breu. [[Detall: Contingut complet i Criteris d'Avaluaci√≥ originals del document]]"`;
  }

  const messages = [
    {
      role: "system",
      content: `${systemPrompt}
      
      INSTRUCCIONS CR√çTIQUES DE FORMAT I CONTINGUT:
      1. T√çTOLS OBLIGATORIS: Genera SEMPRE les 5 seccions exactes llistades amunt.
      2. PERFIL, DIAGN√íSTIC I JUSTIFICACI√ì: Redacta aquestes seccions en format de text seguit (par√†grafs). NO facis llistes verticals. Connecta la justificaci√≥ amb el diagn√≤stic.
      3. ANONIMITZACI√ì: NO incloguis MAI el nom de l'alumne. Substitueix-lo per "L'alumne/a".
      4. DETECCI√ì DE CURS: Busca la llista de cursos i troba la 'X'. Escriu NOM√âS el curs marcat.
      5. MAT√àRIES / ASSIGNATURES: √âs IMPRESCINDIBLE que llistis TOTES les mat√®ries que apareixen a la taula d'adaptacions. Itera per cada fila. Posa un resum de 4-5 paraules fora i TOT el text original (Continguts i Avaluaci√≥) dins del bloc [[Detall: ...]].
      6. CRITERIS D'AVALUACI√ì: Si hi ha criteris generals, posa'ls a la secci√≥ corresponent.
      7. NETEJA FINAL: El document acaba sovint amb signatures, dates, c√†rrecs (Director, Coordinador) i llistes de professionals. Aquesta informaci√≥ NO forma part de "Orientaci√≥ a l'Aula". NO la incloguis al resum. Atura't abans.
      8. NO ASTERISCS: No utilitzis mai asteriscs (*) ni guions (-) al principi de les l√≠nies.
      9. DETALLS: Extreu la frase literal clau del PDF dins dels claud√†tors [[Detall: ...]].
      10. ANTI-AL¬∑LUCINACI√ì: Si no trobes informaci√≥ sobre un punt, no l'escriguis. No omplis buits amb text gen√®ric o inventat.
      
      Processa tot el text proporcionat.`
    },
    {
      role: "user",
      content: `Analitza aquest PI i extreu-ne la informaci√≥ rellevant:\n\n${truncatedText}`
    }
  ];

  try {
    console.log(`ü§ñ [aiService] Enviant petici√≥ a IA Local (http://pi_llm:8080/v1)...`);
    const completion = await openai.chat.completions.create({
      model: MODEL_NAME, // Tornem al model principal (Llama)
      messages: messages,
      temperature: 0.1,
      max_tokens: 2000, // RESTAURAT: 2000 tokens per permetre resums llargs
      stream: true, // ACTIVEM STREAMING per veure el progr√©s
      top_p: 0.9,
      presence_penalty: 0,
      frequency_penalty: 0
    });

    console.log("ü§ñ [aiService] Connexi√≥ establerta amb LLM! Esperant el primer token (Fase de Lectura/Pre-fill)...");

    let fullText = "";
    // Seccions esperades per calcular el progr√©s (aprox 20% per secci√≥)
    // MODIFICAT: Keywords actualitzades segons els nous prompts (Docent/Orientador)
    const sections = ["PERFIL", "DADES", "DIAGN√íSTIC", "ORIENTACI√ì", "ADAPTACIONS", "MAT√àRIES", "ASSIGNATURES", "CRITERIS", "JUSTIFICACI√ì"];
    let isFirst = true;
    let chunkCount = 0;

    for await (const chunk of completion) {
        // FASE 2: ESCRIPTURA (Reset a 0% -> 100%)
        if (isFirst) {
            console.log("ü§ñ [aiService] Primer token rebut! Comen√ßa la generaci√≥ de text.");
            isFirst = false;
            currentProgress = 0; // Reiniciem la barra per a la fase d'escriptura
        }

        chunkCount++;
        const content = chunk.choices[0]?.delta?.content || "";
        
        // Log de "batec" cada 10 chunks per veure que est√† viu a la terminal (M√©s freq√ºent)
        if (chunkCount % 10 === 0) {
            console.log(`... generant (${chunkCount} tokens)`); // M√©s visible als logs de Docker
        }

        fullText += content;

        if (onProgress) {
            // C√†lcul simple de progr√©s: Quantes seccions hem trobat ja?
            let foundCount = 0;
            sections.forEach(s => {
                if (fullText.includes(s)) foundCount++;
            });
            
            // C√†lcul de progr√©s d'escriptura (0 a 100)
            // MODIFICAT: Ajustem a 2000 tokens (resum complet)
            // (chunkCount / 20) -> 2000 tokens = 100%
            const chunkProgress = (chunkCount / 20); 
            const sectionProgress = foundCount * 5; // M√©s pes a les seccions per compensar
            
            let writeProgress = chunkProgress + sectionProgress;
            
            // Enviem text ple -> Servidor marca "GENERANT..."
            // AWAIT IMPORTANT: Esperem que s'actualitzi la BD abans de continuar per evitar race conditions al final
            await onProgress(fullText, Math.min(Math.floor(writeProgress), 99));
        }
    }

    console.log(`ü§ñ [IA Local] Generaci√≥ finalitzada amb √®xit. Longitud: ${fullText.length} car√†cters.`);
    return fullText;
  } catch (error) {
    console.error("‚ùå Error IA Local:", error);
    throw new Error("Error connectant amb el contenidor d'IA Local.");
  }
}

/**
 * Xat r√†pid amb el document.
 * @param {string} text - Text del document
 * @param {string} question - Pregunta de l'usuari
 */
async function chatWithDocument(text, question) {
    // OPTIMITZACI√ì EXTREMA: 1200 chars per velocitat m√†xima al xat
    const MAX_CHARS = 1200; 
    const truncatedText = text.length > MAX_CHARS ? text.substring(0, MAX_CHARS) + "..." : text;

    const messages = [
        {
            role: "system",
            content: `Ets un motor de cerca sem√†ntic.
            TASCA: Interpretar qu√® vol l'usuari i trobar la frase LITERAL del text que ho respon, encara que no faci servir les mateixes paraules.
            
            EXEMPLES:
            - "comportament" -> Busca frases sobre "conducta", "actitud", "normes".
            - "qu√® t√©?" -> Busca "diagn√≤stic", "trastorn", "dificultats".
            
            RESPOSTA: Retorna NOM√âS el fragment de text exacte del document. Si no ho trobes, digues NO_TROBAT.`
        },
        {
            role: "user",
            content: `DOCUMENT:\n"${truncatedText}"\n\nPREGUNTA: "${question}"\n\nRESPOSTA LITERAL DEL DOCUMENT:`
        }
    ];

    try {
        const completion = await openai.chat.completions.create({
            model: MODEL_NAME,
            messages: messages,
            temperature: 0.0, // Determinista (sempre la mateixa resposta)
            max_tokens: 60, // Molt curt (nom√©s volem la frase)
            stream: false 
        });
        return completion.choices[0].message.content;
    } catch (error) {
        console.error("‚ùå Error Chat IA:", error);
        throw new Error("Error connectant amb la IA.");
    }
}

module.exports = { generateSummaryLocal, checkConnection, chatWithDocument };