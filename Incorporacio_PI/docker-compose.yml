services:
  # --- SERVICIO BACKEND ---
  backend:
    build: ./backend
    container_name: pi_backend
    ports:
      - "3001:3001" # Mapeamos puerto 3001 del PC al 3001 del contenedor
    dns:
      - 8.8.8.8
      - 8.8.4.4
    env_file:
      - ./backend/.env
    environment:
      - RABBITMQ_URL=amqp://guest:guest@rabbitmq:5672
    volumes:
      - ./backend:/app # Hot Reload: Lo que edites en tu PC se ve en Docker
      - ./backend/uploads:/app/uploads # Persistencia: Las fotos no se borran al apagar
      - /app/node_modules # Evita que tu node_modules local machaque el de linux

  # --- SERVICIO FRONTEND ---
  frontend:
    build: ./frontend
    container_name: pi_frontend
    ports:
      - "3000:3000" 
    volumes:
      - ./frontend:/app # Hot Reload para Vue
      - /app/node_modules
    environment:
      - CHOKIDAR_USEPOLLING=true # Necesario a veces en Windows/Docker para que detecte cambios
    depends_on:
      - backend # Espera a que el backend arranque un poco

  # --- SERVEI RABBITMQ (Cua de missatges) ---
  rabbitmq:
    image: rabbitmq:3-management
    container_name: pi_rabbitmq
    ports:
      - "5672:5672"   # Port comunicació
      - "15672:15672" # Panell de control web (usuari: guest / guest)
    healthcheck:
      test: ["CMD", "rabbitmqctl", "status"]
      interval: 30s
      timeout: 10s
      retries: 5
    volumes:
      - ./backend/rabbitmq_data:/var/lib/rabbitmq # Persistència de la cua

  # --- SERVEI IA LOCAL (Llama.cpp Server) ---
  llm:
    image: ollama/ollama:latest
    container_name: pi_llm
    volumes:
      - ./backend/models:/models # Mapegem la carpeta on has posat el fitxer .gguf
      - ./backend/ollama_data:/root/.ollama # Persistència d'Ollama
    ports:
      - "11434:11434"
    # --- CONFIGURACIÓ GPU (Descomentar si tens NVIDIA) ---
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: 1
    #           capabilities: [gpu]