version: '3.8'

services:
  # --- BACKEND ---
  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile.dev
    container_name: pi_backend_dev
    ports:
      - "3001:3001"
    env_file:
      - .env
    volumes:
      - ./backend:/app
      - ./backend/uploads:/app/uploads
      - /app/node_modules
    environment:
      # En Docker, llamamos al servicio por su nombre: "ollama"
      - OLLAMA_HOST=http://ollama:11434
    depends_on:
      - rabbitmq
      - ollama

  # --- FRONTEND ---
  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile.dev
    container_name: pi_frontend_dev
    ports:
      - "3000:3000" # Coincide con tu vite.config (server.port)
    volumes:
      - ./frontend:/app
      - /app/node_modules
    environment:
      - CHOKIDAR_USEPOLLING=true
    depends_on:
      - backend

  # --- RABBITMQ (Cola) ---
  rabbitmq:
    image: rabbitmq:3-alpine
    container_name: rabbitmq_dev
    ports:
      - "5672:5672"
      - "15672:15672"

  # --- OLLAMA (IA) ---
  ollama:
    image: ollama/ollama:latest
    container_name: ollama_dev
    ports:
      - "11434:11434" # Exponemos el puerto para que puedas probar la API si quieres
    volumes:
      - ollama_storage_dev:/root/.ollama

volumes:
  ollama_storage_dev: